import torch
import h5py
import json
import numpy as np
from pathlib import Path
from typing import Optional, List, Tuple
from functools import partial
from torch.utils.data import DataLoader, WeightedRandomSampler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

from ..common.base import BaseDomainService
from .trainer import ICDTrainer
from .evaluator import ICDEvaluator
from ....domain.entities import ICDModelEntity
from ....domain.vo import ICDConfig, CNNConfig
from ....foundation.interfaces import TelemetryObserver, Registry, StreamingSource
from ....infrastructure.components import XGBoostStacker
from ....infrastructure.datasource import HDF5StreamingSource, collate_tensor_batch
from ....infrastructure.networks import MedBERTClassifier

class ICDService(BaseDomainService):
    def __init__(
        self, 
        config: ICDConfig,
        registry: Registry, 
        observer: TelemetryObserver,
        train_source: Optional[StreamingSource] = None,
        val_source: Optional[StreamingSource] = None,
        test_source: Optional[StreamingSource] = None
    ):
        super().__init__(observer)
        self.cfg = config
        self.registry = registry
        # These sources are now expected to be HDF5StreamingSource compatible or passed as paths
        self.train_source = train_source 
        self.val_source = val_source
        self.test_source = test_source
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        self.entity: Optional[ICDModelEntity] = None
        self.feats = []
        self.le = None

    def _prepare_data(self):
        """
        Load metadata (classes, features info) from stats.json generated by ETL.
        Unlike previous version, data is already preprocessed in HDF5.
        """
        self.observer.log("INFO", "ICD Service: Loading metadata from cache.")
        cache_dir = Path(self.cfg.data.data_cache_dir)
        stats_path = cache_dir / "stats.json"
        
        if not stats_path.exists():
            raise FileNotFoundError(f"Stats file not found at {stats_path}. Run ETL TensorConverter first.")

        with open(stats_path, "r") as f:
            stats = json.load(f)
            
        # Reconstruct LabelEncoder
        if "icd_classes" in stats:
            self.le = LabelEncoder()
            self.le.classes_ = np.array(stats["icd_classes"])
            self.observer.log("INFO", f"ICD Service: Loaded {len(self.le.classes_)} classes from metadata.")
        else:
            raise ValueError("icd_classes not found in stats.json")

        # In HDF5 pipeline, we can infer feature count from the file or config
        # Here we assume we can read it from the train source if initialized
        num_feats = 0
        if isinstance(self.train_source, HDF5StreamingSource):
             # Initialize file to read shape
             _ = self.train_source[0] 
             num_feats = self.train_source.h5_file["X_num"].shape[1]
             self.feats = [f"feat_{i}" for i in range(num_feats)] # Dummy names

        return self.feats, "y", self.le

    def execute(self) -> None:
        """
        Orchestrates the training process using HDF5 sources:
        1. Load Metadata
        2. Initialize Models
        3. Setup DataLoader & Sampler (reading from HDF5)
        4. Run Trainer
        """
        feats, label_col, le = self._prepare_data()
        
        # Assuming train_source is HDF5StreamingSource initialized by factory/assembler
        if not isinstance(self.train_source, HDF5StreamingSource):
             # Fallback if just paths were passed or need re-init
             prefix = self.cfg.data.input_h5_prefix # e.g. train_coding
             cache_dir = Path(self.cfg.data.data_cache_dir)
             self.train_source = HDF5StreamingSource(cache_dir / f"{prefix}_train.h5", label_key="y")
             self.val_source = HDF5StreamingSource(cache_dir / f"{prefix}_val.h5", label_key="y")

        num_feats = len(feats) if feats else self.train_source[0].x_num.shape[0]
        num_classes = len(le.classes_)
        
        head1 = MedBERTClassifier(self.cfg.model_name, num_num=num_feats, num_cls=num_classes)
        head2 = MedBERTClassifier(self.cfg.model_name, num_num=num_feats, num_cls=num_classes)
        stacker = XGBoostStacker(self.cfg.xgb_params)
        
        self.entity = ICDModelEntity(head1, head2, stacker)

        # Sampler weights calculation from HDF5 'y'
        self.observer.log("INFO", "ICD Service: Calculating sampler weights from HDF5...")
        with h5py.File(self.train_source.h5_path, "r") as f:
             y_all = f["y"][:]
        
        target_counts = np.bincount(y_all)
        # Handle sparse classes (some might be 0)
        class_weights_map = {}
        for cls, count in enumerate(target_counts):
            if count > 0:
                class_weights_map[cls] = 1.0 / (count ** self.cfg.sampler_alpha)
            else:
                class_weights_map[cls] = 1.0

        sample_weights = torch.tensor([class_weights_map.get(x, 1.0) for x in y_all], dtype=torch.double)
        sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)

        # HDF5StreamingSource returns TensorBatch, so we use collate_tensor_batch
        collate_fn = collate_tensor_batch

        dl_tr = DataLoader(self.train_source, batch_size=self.cfg.batch_size, sampler=sampler, collate_fn=collate_fn, num_workers=self.cfg.num_workers)
        dl_val = DataLoader(self.val_source, batch_size=self.cfg.batch_size, collate_fn=collate_fn, num_workers=self.cfg.num_workers)

        class_freq = target_counts 
        
        trainer = ICDTrainer(self.cfg, self.entity, self.registry, self.observer, self.device, class_freq)
        evaluator = ICDEvaluator(self.cfg, self.entity, self.registry, self.observer, self.device)
        
        trainer.train(dl_tr, dl_val, evaluator)

    def _ensure_entity_loaded(self) -> bool:
        if self.entity is not None:
            return True

        try:
            state = self.registry.load_model(self.cfg.artifacts.model_name, "best", str(self.device))
            
            # Load metadata to get num_classes
            feats, _, le = self._prepare_data()
            if not feats and isinstance(self.train_source, HDF5StreamingSource):
                 _ = self.train_source[0]
                 num_feats = self.train_source.h5_file["X_num"].shape[1]
            else:
                 num_feats = len(feats)

            num_classes = len(le.classes_)
            head1 = MedBERTClassifier(self.cfg.model_name, num_num=num_feats, num_cls=num_classes)
            head2 = MedBERTClassifier(self.cfg.model_name, num_num=num_feats, num_cls=num_classes)
            stacker = XGBoostStacker(self.cfg.xgb_params)
            self.entity = ICDModelEntity(head1, head2, stacker)
            self.entity.load_state_dict(state)
            self.entity.to(str(self.device))
            self.observer.log("INFO", "ICD Service: Model state reloaded for inference.")
            return True
        except Exception as exc:
            self.observer.log("WARNING", f"ICD Service: Unable to load saved model ({exc}); ensure train workflow completed.")
        return False

    def generate_intervention_dataset(self, cnn_config: CNNConfig) -> None:
        """
        Run inference on the coding dataset (H5) and emit enriched H5 artifacts for the intervention task.
        Swaps 'y' (ICD) with 'y_vent' (Ventilation) in the output.
        """
        self.observer.log("INFO", "ICD Service: Starting intervention dataset generation.")

        if not self._ensure_entity_loaded():
            return

        self.entity.head1.eval()
        self.entity.head2.eval()
        self.entity.to(str(self.device))

        cache_dir = Path(self.cfg.data.data_cache_dir or cnn_config.data.data_cache_dir)
        input_prefix = self.cfg.data.input_h5_prefix # e.g., train_coding
        output_prefix = self.cfg.data.output_h5_prefix # e.g., train_intervention

        target_files = [f"{input_prefix}_{split}.h5" for split in ["train", "val", "test"]]
        output_dim = self.entity.head1.fc.out_features

        for fname in target_files:
            src_path = cache_dir / fname
            if not src_path.exists():
                self.observer.log("WARNING", f"ICD Service: Skipping missing cache file {src_path.name}.")
                continue

            split_name = fname.replace(f"{input_prefix}_", "").replace(".h5", "")
            dst_path = cache_dir / f"{output_prefix}_{split_name}.h5"
            self.observer.log("INFO", f"ICD Service: Augmenting {src_path.name} -> {dst_path.name}")

            with h5py.File(src_path, "r") as src, h5py.File(dst_path, "w") as dst:
                # Copy existing datasets
                for key in src.keys():
                    if key in ("X_icd", self.cfg.data.inferred_col_name, "y", "y_vent"):
                        continue
                    dst.copy(src[key], key)

                # SWAP LABELS: Intervention task needs Ventilation label as 'y'
                if "y_vent" in src:
                    dst.copy(src["y_vent"], "y")
                else:
                    self.observer.log("WARNING", "y_vent not found in source H5, retaining original y (might be wrong label for CNN).")
                    dst.copy(src["y"], "y")

                # Prepare for Inference
                sids = src["sid"][:]
                total = len(sids)
                prob_ds = dst.create_dataset("X_icd", (total, output_dim), dtype=np.float32)
                code_ds = dst.create_dataset(self.cfg.data.inferred_col_name, (total,), dtype=np.int64)
                
                # Check for input_ids in H5
                if "input_ids" not in src or "X_num" not in src:
                    self.observer.log("ERROR", "Missing input_ids or X_num in source H5. Cannot run inference.")
                    continue

                chunk_size = 512
                for i in range(0, total, chunk_size):
                    end_idx = min(i + chunk_size, total)
                    
                    # Load batch from H5
                    b_ids = torch.from_numpy(src["input_ids"][i:end_idx]).long().to(self.device)
                    b_mask = torch.from_numpy(src["attention_mask"][i:end_idx]).long().to(self.device)
                    b_num = torch.from_numpy(src["X_num"][i:end_idx]).float().to(self.device)

                    with torch.no_grad():
                        o1 = self.entity.head1(b_ids, b_mask, b_num)
                        o2 = self.entity.head2(b_ids, b_mask, b_num)
                        avg_logits = (o1 + o2) / 2
                        probs = torch.softmax(avg_logits, dim=-1)

                    prob_vec = probs.cpu().numpy()
                    argmax_vec = prob_vec.argmax(axis=1)

                    prob_ds[i:end_idx] = prob_vec
                    code_ds[i:end_idx] = argmax_vec

        self.observer.log("INFO", "ICD Service: Augmentation complete.")