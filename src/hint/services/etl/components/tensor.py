import polars as pl
import numpy as np
import h5py
from pathlib import Path
from typing import List
from ....foundation.interfaces import PipelineComponent, Registry, TelemetryObserver
from ....domain.vo import ETLConfig, ICDConfig, CNNConfig

class TensorConverter(PipelineComponent):
    """Convert assembled features into HDF5 tensors for model training.
    
    Transforms the flat tabular format into 3D tensors (Batch, Channel, Time) 
    compatible with PyTorch training loops.
    """

    def __init__(self, etl_config: ETLConfig, cnn_config: CNNConfig, icd_config: ICDConfig, registry: Registry, observer: TelemetryObserver):
        self.etl_cfg = etl_config
        self.cnn_cfg = cnn_config
        self.icd_cfg = icd_config
        self.registry = registry
        self.observer = observer

    def execute(self) -> None:
        """Process features and labels into HDF5 tensors."""
        proc_dir = Path(self.etl_cfg.proc_dir)
        # Use ICD config for data cache as it is the primary consumer of the first stage output
        cache_dir = Path(self.icd_cfg.data.data_cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)

        features_path = proc_dir / self.etl_cfg.artifacts.features_file
        
        # Load correct target files generated by LabelGenerator
        icd_targets_path = proc_dir / self.etl_cfg.artifacts.icd_targets_file
        vent_targets_path = proc_dir / self.etl_cfg.artifacts.vent_targets_file

        if not features_path.exists():
            raise FileNotFoundError(f"Missing features artifact: {features_path}")
        
        self.observer.log("INFO", f"TensorConverter: Stage 1/4 Loading features from {features_path}")
        
        # Load Features
        df = pl.read_parquet(features_path)
        
        # Determine Feature Columns
        cols = df.columns
        numeric_cols = [c for c in cols if c.startswith("V__")]
        categorical_cols = [c for c in cols if c.startswith("S__")]
        
        self.observer.log("INFO", f"TensorConverter: Found {len(numeric_cols)} dynamic features and {len(categorical_cols)} static features.")

        # Stage 2: Join Targets
        self.observer.log("INFO", "TensorConverter: Stage 2/4 Joining Targets")
        
        # 1. Join ICD Targets (target_icd -> y)
        if icd_targets_path.exists():
            self.observer.log("INFO", f"TensorConverter: Loading ICD targets from {icd_targets_path.name}")
            icd_df = pl.read_parquet(icd_targets_path)
            
            # Rename 'target_icd' to 'y' to match HDF5/Model expectation
            if "target_icd" in icd_df.columns:
                icd_df = icd_df.rename({"target_icd": "y"})
            
            # Join on ICUSTAY_ID (icd_targets usually has ICUSTAY_ID and target)
            df = df.join(icd_df, on="ICUSTAY_ID", how="left")
        else:
            self.observer.log("WARNING", f"TensorConverter: ICD targets not found at {icd_targets_path}. 'y' will be missing.")

        # 2. Join Vent Targets (target_vent -> y_vent)
        if vent_targets_path.exists():
            self.observer.log("INFO", f"TensorConverter: Loading Vent targets from {vent_targets_path.name}")
            vent_df = pl.read_parquet(vent_targets_path)
            
            # Rename 'target_vent' to 'y_vent'
            if "target_vent" in vent_df.columns:
                vent_df = vent_df.rename({"target_vent": "y_vent"})
            
            # Join on ICUSTAY_ID and HOUR_IN
            # Select only necessary columns to avoid conflicts
            join_keys = ["ICUSTAY_ID", "HOUR_IN"]
            vent_df = vent_df.select(join_keys + ["y_vent"])
            
            df = df.join(vent_df, on=join_keys, how="left")

        # Determine Splits
        self.observer.log("INFO", "TensorConverter: Stage 3/4 Splitting dataset")
        
        subjects = df.select("SUBJECT_ID").unique()
        n_subjects = subjects.height
        n_train = int(n_subjects * 0.7)
        n_val = int(n_subjects * 0.1)
        
        # Shuffle deterministically
        subjects = subjects.sample(fraction=1.0, shuffle=True, seed=42)
        
        train_ids = subjects[:n_train]["SUBJECT_ID"]
        val_ids = subjects[n_train:n_train+n_val]["SUBJECT_ID"]
        test_ids = subjects[n_train+n_val:]["SUBJECT_ID"]
        
        prefix = self.icd_cfg.data.input_h5_prefix 
        max_cands_len = 50 

        self.observer.log("INFO", f"TensorConverter: Stage 4/4 Writing HDF5 splits with prefix '{prefix}'")
        
        # Filter using "SUBJECT_ID" not "ICUSTAY_ID"
        self._process_split(df.filter(pl.col("SUBJECT_ID").is_in(train_ids)), "train", numeric_cols, categorical_cols, cache_dir, prefix, max_cands_len)
        self.observer.log("INFO", f"TensorConverter: Writing val split to {cache_dir / f'{prefix}_val.h5'}")
        self._process_split(df.filter(pl.col("SUBJECT_ID").is_in(val_ids)), "val", numeric_cols, categorical_cols, cache_dir, prefix, max_cands_len)
        self.observer.log("INFO", f"TensorConverter: Writing test split to {cache_dir / f'{prefix}_test.h5'}")
        self._process_split(df.filter(pl.col("SUBJECT_ID").is_in(test_ids)), "test", numeric_cols, categorical_cols, cache_dir, prefix, max_cands_len)

    def _process_split(self, split_df: pl.DataFrame, split_name: str, num_cols: List[str], cat_cols: List[str], cache_dir: Path, prefix: str, max_cands: int):
        if split_df.height == 0:
            self.observer.log("WARNING", f"TensorConverter: Split {split_name} is empty. Skipping.")
            return

        n_samples = split_df.select("ICUSTAY_ID").n_unique()
        seq_len = self.cnn_cfg.seq_len
        n_features = len(num_cols)
        
        out_path = cache_dir / f"{prefix}_{split_name}.h5"
        
        with h5py.File(out_path, "w") as f:
            # Create input dataset (Batch, Channel, Time)
            # PyTorch models usually expect (Batch, Channels, Time)
            f.create_dataset("X_num", (n_samples, n_features, seq_len), dtype='f4')
            
            # [FIX] Create static features dataset (Batch, Features)
            if cat_cols:
                f.create_dataset("X_cat", (n_samples, len(cat_cols)), dtype='i4')
            
        self.observer.log("INFO", f"TensorConverter: Vectorized processing for {n_samples} patients...")
        
        sorted_df = split_df.sort(["ICUSTAY_ID", "HOUR_IN"])
        unique_stays = sorted_df.select("ICUSTAY_ID").unique(maintain_order=True)
        stay_ids = unique_stays["ICUSTAY_ID"].to_list()
        
        # Aggregate features for ICD Targets (Static per stay)
        agg_exprs = []
        if "y" in sorted_df.columns:
            agg_exprs.append(pl.col("y").first())
        
        # [FIX] Collect static categorical values (taking first value per stay)
        if cat_cols:
            agg_exprs.extend([pl.col(c).first() for c in cat_cols])
            
        grouped = sorted_df.group_by("ICUSTAY_ID", maintain_order=True).agg(agg_exprs)
        
        # Pre-allocate Arrays: (Batch, Time, Features)
        X_num = np.zeros((n_samples, seq_len, n_features), dtype=np.float32)
        y_vent = np.zeros((n_samples, seq_len), dtype=np.float32)
        y_vaso = np.zeros((n_samples, seq_len), dtype=np.float32)
        
        # Filter to valid time window 0..seq_len-1
        valid_window = sorted_df.filter((pl.col("HOUR_IN") >= 0) & (pl.col("HOUR_IN") < seq_len))
        
        # Map ICUSTAY_ID to 0..N index
        id_map = {sid: i for i, sid in enumerate(stay_ids)}
        
        # Extract columns as numpy
        stay_col = valid_window["ICUSTAY_ID"].to_numpy()
        hour_col = valid_window["HOUR_IN"].to_numpy()
        
        # Calculate indices
        row_indices = np.array([id_map[s] for s in stay_col])
        col_indices = hour_col
        
        # Fill X_num
        for i, col_name in enumerate(num_cols):
            vals = valid_window[col_name].to_numpy().astype(np.float32)
            X_num[row_indices, col_indices, i] = vals
            
        # Fill Targets
        # Use 'y_vent' if joined from targets, else 'VENT' from raw
        vent_col = "y_vent" if "y_vent" in valid_window.columns else "VENT"
        if vent_col in valid_window.columns:
            vals = valid_window[vent_col].to_numpy().astype(np.float32)
            y_vent[row_indices, col_indices] = vals
            
        if "VASO" in valid_window.columns:
            vals = valid_window["VASO"].to_numpy().astype(np.float32)
            y_vaso[row_indices, col_indices] = vals
            
        # Transpose X_num to (Batch, Channel, Time) for PyTorch
        X_num_transposed = np.transpose(X_num, (0, 2, 1))
        
        with h5py.File(out_path, "a") as f:
            f["X_num"][:] = X_num_transposed
            
            # [FIX] Write Static Features
            if cat_cols:
                # Grouped dataframe is already sorted by ICUSTAY_ID same as stay_ids order
                # because we used maintain_order=True and sort upfront.
                x_cat_data = grouped.select(cat_cols).to_numpy().astype(np.int32)
                f["X_cat"][:] = x_cat_data
            
            # Write Targets
            if vent_col in valid_window.columns:
                f.create_dataset("y_vent", data=y_vent)
            if "VASO" in valid_window.columns:
                f.create_dataset("y_vaso", data=y_vaso)
                
            # Write ICD Targets ('y')
            if "y" in grouped.columns:
                try:
                    # Convert to numpy
                    y_list = grouped["y"].to_list()
                    # Determine type: int (indices) or float/list (multi-hot)
                    
                    # Try creating array
                    icd_targets = np.array(y_list)
                    
                    if icd_targets.dtype == object:
                        try:
                            # Try stacking if list of lists (Multi-hot)
                            icd_targets = np.vstack(y_list).astype(np.float32)
                            f.create_dataset("y", data=icd_targets)
                        except:
                            self.observer.log("WARNING", "ICD targets are ragged/invalid. Skipping 'y'.")
                    elif np.issubdtype(icd_targets.dtype, np.integer):
                        # Save as int64 for bincount/CrossEntropyLoss indices
                        f.create_dataset("y", data=icd_targets.astype(np.int64), dtype='i8')
                    else:
                        # Float (e.g. Multi-hot or Soft labels)
                        f.create_dataset("y", data=icd_targets.astype(np.float32))
                        
                except Exception as e:
                    self.observer.log("WARNING", f"Could not vectorize ICD targets: {e}")
            
            # Save Stay IDs for reference
            f.create_dataset("stay_ids", data=np.array(stay_ids, dtype=np.int64))