data:
  input_h5_prefix: "train_coding"
  output_h5_prefix: "train_intervention"
  inferred_col_name: "icd_inferred_code"
  data_cache_dir: "data/cache"

artifacts:
  model_name: "icd_model"
  stacker_name: "icd_stacker"

model_name: "Charangan/MedBERT"

# Training
batch_size: 2048
lr: 1.0e-5
epochs: 0
patience: 5
dropout: 0.3

# Resources
num_workers: 4
pin_memory: true
max_length: 32

# Validation & XAI
test_split_size: 0.2
val_split_size: 0.125
top_k_labels: 500
topk_eval: 5

xai_bg_size: 128
xai_sample_size: 5
xai_nsamples: 200

# Loss / Imbalance
sampler_alpha: 0.5
cb_beta: 0.999
focal_gamma: 1.5
logit_adjust_tau: 1.0
entropy_reg_lambda: 1.0e-3
freeze_bert_epochs: 1

pca_components: 0.95
xgb_params:
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  tree_method: "hist"
  objective: "multi:softprob"
