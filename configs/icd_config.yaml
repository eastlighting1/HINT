data:
  input_h5_prefix: "train_coding"
  output_h5_prefix: "train_intervention"
  inferred_col_name: "icd_inferred_code"
  data_cache_dir: "data/cache"

execution:
  subset_ratio: 0.1

artifacts:
  model_name: "icd_model"
  stacker_name: "icd_stacker"

# Execution Strategy
models_to_run:
  - "GRU-D"
  - "TST"
  - "LatentODE"
  - "iTransformer"

# Common Hyperparameters
dropout: 0.3
max_length: 32  # Seq len for Time Series / Max len for Text

# Model Specific Configurations
model_configs:
  MedBERT:
    bert_model_name: "Charangan/MedBERT"
  
  GRU-D:
    hidden_dim: 128
    decay_rate: 0.99
  
  TST:
    d_model: 128
    n_heads: 4
    num_layers: 3
    d_ff: 256
  
  LatentODE:
    latent_dim: 64
    rec_dim: 128
    ode_solver: "rk4"
  
  PatchTST:
    d_model: 128
    n_heads: 4
    num_layers: 3
    d_ff: 256
    patch_len: 8
    stride: 4
  
  iTransformer:
    d_model: 128
    n_heads: 4
    num_layers: 3
    d_ff: 256

# Training
batch_size: 1024  # Reduced for stability with complex models
lr: 1.0e-4
epochs: 100
patience: 5

# Resources
num_workers: 8
pin_memory: true

# Validation & XAI
test_split_size: 0.2
val_split_size: 0.125
top_k_labels: 500
topk_eval: 5

xai_bg_size: 128
xai_sample_size: 5
xai_nsamples: 200

# Loss / Imbalance
sampler_alpha: 0.5
cb_beta: 0.999
focal_gamma: 1.5
logit_adjust_tau: 1.0
entropy_reg_lambda: 1.0e-3
freeze_bert_epochs: 1

pca_components: 0.95
xgb_params:
  n_estimators: 100
  max_depth: 6
  learning_rate: 0.1
  tree_method: "hist"
  objective: "multi:softprob"