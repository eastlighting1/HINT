data:
  input_h5_prefix: "train_coding"
  data_cache_dir: "data/cache"

artifacts:
  model_name: "icd_model"

model_testing: false
models_to_run:
  - "TabNet"
  - "DCNv2"

dropout: 0.3

loss_type: "adaptive_clpl"
adaptive_clpl_head_size: 800
adaptive_clpl_tail_sample_size: 800
adaptive_clpl_logit_clip: 30.0
lambda_sparse: 1.0e-3

pseudo_label_ce_weight: 0.2
pseudo_label_margin: 0.1
class_weight_power: 1.0
class_weight_clip: 10.0

model_configs:
  MedBERT:
    bert_model_name: "Charangan/MedBERT"

  GRU-D:
    hidden_dim: 128
    decay_rate: 0.99

  TST:
    d_model: 128
    n_heads: 4
    num_layers: 3
    d_ff: 256

  LatentODE:
    latent_dim: 64
    rec_dim: 128
    ode_solver: "rk4"

  iTransformer:
    d_model: 128
    n_heads: 4
    num_layers: 3
    d_ff: 256

  TabNet:
    n_d: 64
    n_a: 64
    n_steps: 3
    gamma: 1.3
    n_independent: 2
    n_shared: 2
    virtual_batch_size: 64
    momentum: 0.02
    proj_dim: 512

  DCNv2:
    cross_layers: 2
    deep_layers: [256, 256, 256]
    dropout: 0.2
    lr: 5.0e-5
    cross_scale: 0.1
    proj_dim: 512

  FTTransformer:
    d_token: 192
    n_blocks: 3
    n_heads: 8
    d_ffn: 256

batch_size: 128
lr: 5.0e-5
lr_plateau_factor: 0.5
lr_plateau_patience: 2
lr_plateau_min_lr: 1.0e-6
epochs: 100
patience: 5
use_amp: false
grad_clip_norm: 1.0

num_workers: 8
pin_memory: true

top_k_labels: 1000
